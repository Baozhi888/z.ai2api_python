#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
from typing import Dict, List, Optional
from pydantic_settings import BaseSettings
from app.utils.logger import logger


class Settings(BaseSettings):
    """Application settings"""

    # API Configuration
    API_ENDPOINT: str = os.getenv("API_ENDPOINT", "https://chat.z.ai/api/chat/completions")
    AUTH_TOKEN: str = os.getenv("AUTH_TOKEN", "sk-your-api-key")

    # ËÆ§ËØÅtokenÊñá‰ª∂Ë∑ØÂæÑ
    AUTH_TOKENS_FILE: str = os.getenv("AUTH_TOKENS_FILE", "tokens.txt")

    # TokenÊ±†ÈÖçÁΩÆ
    TOKEN_HEALTH_CHECK_INTERVAL: int = int(os.getenv("TOKEN_HEALTH_CHECK_INTERVAL", "300"))  # 5ÂàÜÈíü
    TOKEN_FAILURE_THRESHOLD: int = int(os.getenv("TOKEN_FAILURE_THRESHOLD", "3"))  # Â§±Ë¥•3Ê¨°ÂêéÊ†áËÆ∞‰∏∫‰∏çÂèØÁî®
    TOKEN_RECOVERY_TIMEOUT: int = int(os.getenv("TOKEN_RECOVERY_TIMEOUT", "1800"))  # 30ÂàÜÈíüÂêéÈáçËØïÂ§±Ë¥•ÁöÑtoken

    def _load_tokens_from_file(self, file_path: str) -> List[str]:
        """
        ‰ªéÊñá‰ª∂Âä†ËΩΩtokenÂàóË°®

        ÊîØÊåÅ‰∏§ÁßçÊ†ºÂºèÔºö
        1. ÊØèË°å‰∏Ä‰∏™tokenÔºàÂéüÊ†ºÂºèÔºâ
        2. ÈÄóÂè∑ÂàÜÈöîÁöÑtokenÔºàÊñ∞Ê†ºÂºèÔºâ

        Â§ÑÁêÜËßÑÂàôÔºö
        - Ë∑≥ËøáÁ©∫Ë°åÂíåÊ≥®ÈáäË°åÔºà‰ª•#ÂºÄÂ§¥Ôºâ
        - Ëá™Âä®Ê£ÄÊµãÂπ∂Â§ÑÁêÜÈÄóÂè∑ÂàÜÈöîÊ†ºÂºè
        - ÂéªÈô§Á©∫Ê†ºÂíåÊç¢Ë°åÁ¨¶
        """
        tokens = []
        try:
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()

                    if not content:
                        logger.debug(f"üìÑ TokenÊñá‰ª∂‰∏∫Á©∫: {file_path}")
                        return tokens

                    # Ê£ÄÊü•ÊòØÂê¶ÂåÖÂê´ÈÄóÂè∑ÂàÜÈöîÊ†ºÂºè
                    if ',' in content:
                        # ÈÄóÂè∑ÂàÜÈöîÊ†ºÂºèÔºöÂ∞ÜÊï¥‰∏™Êñá‰ª∂ÂÜÖÂÆπÊåâÈÄóÂè∑ÂàÜÂâ≤
                        logger.debug(f"üìÑ Ê£ÄÊµãÂà∞ÈÄóÂè∑ÂàÜÈöîÊ†ºÂºè: {file_path}")

                        # ÁßªÈô§Ê≥®ÈáäË°åÂêéÂÜçÂàÜÂâ≤
                        lines = content.split('\n')
                        clean_content = []
                        for line in lines:
                            line = line.strip()
                            if line and not line.startswith('#'):
                                clean_content.append(line)

                        # ÂêàÂπ∂ÊâÄÊúâÈùûÊ≥®ÈáäÂÜÖÂÆπÔºåÁÑ∂ÂêéÊåâÈÄóÂè∑ÂàÜÂâ≤
                        merged_content = ' '.join(clean_content)
                        raw_tokens = merged_content.split(',')

                        for token in raw_tokens:
                            token = token.strip()
                            if token:  # Ë∑≥ËøáÁ©∫token
                                tokens.append(token)
                    else:
                        # ÊØèË°å‰∏Ä‰∏™tokenÊ†ºÂºèÔºàÂéüÊ†ºÂºèÔºâ
                        logger.debug(f"üìÑ ‰ΩøÁî®ÊØèË°å‰∏Ä‰∏™tokenÊ†ºÂºè: {file_path}")
                        for line in content.split('\n'):
                            line = line.strip()
                            # Ë∑≥ËøáÁ©∫Ë°åÂíåÊ≥®ÈáäË°å
                            if line and not line.startswith('#'):
                                tokens.append(line)

                logger.info(f"üìÑ ‰ªéÊñá‰ª∂Âä†ËΩΩ‰∫Ü {len(tokens)} ‰∏™token: {file_path}")
            else:
                logger.debug(f"üìÑ TokenÊñá‰ª∂‰∏çÂ≠òÂú®: {file_path}")
        except Exception as e:
            logger.error(f"‚ùå ËØªÂèñtokenÊñá‰ª∂Â§±Ë¥• {file_path}: {e}")
        return tokens

    @property
    def auth_token_list(self) -> List[str]:
        """
        Ëß£ÊûêËÆ§ËØÅtokenÂàóË°®

        ‰ªÖ‰ªéAUTH_TOKENS_FILEÊåáÂÆöÁöÑÊñá‰ª∂Âä†ËΩΩtoken
        """
        # ‰ªéÊñá‰ª∂Âä†ËΩΩtoken
        tokens = self._load_tokens_from_file(self.AUTH_TOKENS_FILE)

        # ÂéªÈáçÔºå‰øùÊåÅÈ°∫Â∫è
        if tokens:
            seen = set()
            unique_tokens = []
            for token in tokens:
                if token not in seen:
                    unique_tokens.append(token)
                    seen.add(token)

            # ËÆ∞ÂΩïÂéªÈáç‰ø°ÊÅØ
            duplicate_count = len(tokens) - len(unique_tokens)
            if duplicate_count > 0:
                logger.warning(f"‚ö†Ô∏è Ê£ÄÊµãÂà∞ {duplicate_count} ‰∏™ÈáçÂ§çtokenÔºåÂ∑≤Ëá™Âä®ÂéªÈáç")

            return unique_tokens

        return []

    # Model Configuration
    PRIMARY_MODEL: str = os.getenv("PRIMARY_MODEL", "GLM-4.5")
    THINKING_MODEL: str = os.getenv("THINKING_MODEL", "GLM-4.5-Thinking")
    SEARCH_MODEL: str = os.getenv("SEARCH_MODEL", "GLM-4.5-Search")
    AIR_MODEL: str = os.getenv("AIR_MODEL", "GLM-4.5-Air")

    # Server Configuration
    LISTEN_PORT: int = int(os.getenv("LISTEN_PORT", "8080"))
    DEBUG_LOGGING: bool = os.getenv("DEBUG_LOGGING", "true").lower() == "true"

    ANONYMOUS_MODE: bool = os.getenv("ANONYMOUS_MODE", "true").lower() == "true"
    TOOL_SUPPORT: bool = os.getenv("TOOL_SUPPORT", "true").lower() == "true"
    SCAN_LIMIT: int = int(os.getenv("SCAN_LIMIT", "200000"))
    SKIP_AUTH_TOKEN: bool = os.getenv("SKIP_AUTH_TOKEN", "false").lower() == "true"

    # Retry Configuration
    MAX_RETRIES: int = int(os.getenv("MAX_RETRIES", "5"))
    RETRY_DELAY: float = float(os.getenv("RETRY_DELAY", "1.0"))  # ÂàùÂßãÈáçËØïÂª∂ËøüÔºàÁßíÔºâ
    RETRY_BACKOFF: float = float(os.getenv("RETRY_BACKOFF", "2.0"))  # ÈÄÄÈÅøÁ≥ªÊï∞

    # Browser Headers
    CLIENT_HEADERS: Dict[str, str] = {
        "Content-Type": "application/json",
        "Accept": "application/json, text/event-stream",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0",
        "Accept-Language": "zh-CN",
        "sec-ch-ua": '"Not;A=Brand";v="99", "Microsoft Edge";v="139", "Chromium";v="139"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "X-FE-Version": "prod-fe-1.0.70",
        "Origin": "https://chat.z.ai",
    }

    class Config:
        env_file = ".env"


settings = Settings()
